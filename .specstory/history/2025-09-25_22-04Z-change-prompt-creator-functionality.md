<!-- Generated by SpecStory -->

# Change prompt creator functionality (2025-09-25 22:04Z)

_**User (2025-09-26 11:28Z)**_

Can we change how the prompt creator works in human mode?

I want to load the AI_csv into intial prompt and the human answers into the refinem prompt.

---

_**Assistant**_

Read file: undefined

---

<details><summary>Edit file: /home/codyt/Documents/Projects/Cognitive-Assistant/Existential Layer/prompt_creator.py</summary>

undefined

**Chunk 1**
Lines added: 38, lines removed: 83

```diff
@@ -537,113 +537,68 @@
  
  def create_human_interview_prompts():
-     """Create system prompts using human interview data as foundation, refined with AI responses.
- 
+     """Create system prompts using AI-generated responses as foundation, refined with human interview data.
+     
      Process:
-     1. Load human interview responses (authentic foundation)
-     2. Generate initial existential layer summary from human data only
-     3. Load newest AI-generated responses (from question_asker.py --model songbird)
-     4. Refine the human-based summary using AI responses for deeper insights
+     1. Load AI-generated responses (initial foundation)
+     2. Generate initial existential layer summary from AI data only
+     3. Load human interview responses
+     4. Refine the AI-based summary using human responses for authentic grounding
      """
-     print("ğŸ‘¤ Creating Human Interview + AI Refined System Prompts...")
-     print("   Step 1: Generate initial summary from human responses")
-     print("   Step 2: Refine with AI-generated responses using human context")
- 
-     # Load human interview data (foundation)
-     human_context = load_human_interview_data()
- 
-     # Load the newest AI-generated responses for refinement
+     print("ğŸ¤– Creating AI + Human Refined System Prompts...")
+     print("   Step 1: Generate initial summary from AI responses")
+     print("   Step 2: Refine with human interview responses")
+     
+     # Load AI-generated responses (foundation)
      try:
          ai_csv = get_most_recent_file("questions_with_answers_songbird_*.csv")
-         print(f"ğŸ“ Loading AI refinement data: {os.path.basename(ai_csv)}")
+         print(f"ğŸ“ Loading AI foundation data: {os.path.basename(ai_csv)}")
      except FileNotFoundError:
-         raise FileNotFoundError("No AI-generated response files found for refinement")
- 
-     # Load AI responses using standard CSV method
-     def load_ai_responses(csv_path: Path) -> str:
-         """Load AI responses for refinement."""
-         with open(csv_path, 'r', encoding='utf-8') as f:
-             reader = csv.DictReader(
-                 f,
-                 delimiter=config.csv.DELIMITER,
-                 quotechar=config.csv.QUOTECHAR,
-                 fieldnames=config.csv.FIELDNAMES
-             )
-             # Convert to list and skip header
-             data = list(reader)[1:]
- 
-         # Format as readable text for LLM - process CSV data directly
-         formatted_sections = []
-         for row in data:
-             category = row.get('Category', '').strip()
-             if not category:
-                 continue
- 
-             questions = []
-             answers = []
- 
-             # Extract Q&A pairs directly from CSV columns
-             for i in range(1, 4):  # Questions 1-3, Answers 1-3
-                 question_key = f'Question {i}'
-                 answer_key = f'Human_Answer {i}'
- 
-                 question = row.get(question_key, '').strip()
-                 answer = row.get(answer_key, '').strip()
- 
-                 if question and answer:
-                     questions.append(question)
-                     answers.append(answer)
- 
-             if questions and answers:
-                 qa_pairs = []
-                 for q, a in zip(questions, answers):
-                     qa_pairs.append(f"Q: {q}\nA: {a}")
-                 formatted_entry = f"## {category}\n" + "\n\n".join(qa_pairs)
-                 formatted_sections.append(formatted_entry)
- 
-         return "\n\n".join(formatted_sections)
- 
+         raise FileNotFoundError("No AI-generated response files found for foundation")
+     
      ai_context = load_ai_responses(ai_csv)
-     print(f"ğŸ“Š Loaded AI refinement data: {len(ai_context)} characters")
- 
+     
+     # Load human interview data for refinement
+     human_context = load_human_interview_data()
+     
      # Store prompt templates for direct use
      initial_summary_template = config.prompts.create_initial_prompt
      refine_template = config.prompts.refine_template
- 
-     # Define the state of the graph - HUMAN + AI REFINEMENT VERSION
+     
+     # Define the state of the graph - AI + HUMAN REFINEMENT VERSION
      class State(TypedDict):
-         human_context: str
          ai_context: str
+         human_context: str
          refinement_step: int
          summary: str
- 
-     # Generate initial existential layer summary from HUMAN interview content
-     async def generate_initial_summary(human_context: str) -> str:
-         print("ğŸ“ Step 1: Generating initial existential layer summary from HUMAN responses...")
- 
+     
+     # Generate initial existential layer summary from AI responses
+     async def generate_initial_summary(ai_context: str) -> str:
+         print("ğŸ“ Step 1: Generating initial existential layer summary from AI responses...")
+         
          # Process context with token-based chunking
          config_params = {}
          summary = await process_large_context(
-             context=human_context,
+             context=ai_context,
              prompt_template=initial_summary_template,
              config_params=config_params,
              max_tokens=30000
          )
- 
+         
          return summary
- 
-     # Step 2: Refine with AI-generated responses
-     async def refine_with_ai_responses(current_summary: str, ai_context: str) -> str:
-         print("ğŸ¯ Step 2: Refining with AI-generated responses using human context...")
- 
+     
+     # Step 2: Refine with human interview responses
+     async def refine_with_human_responses(current_summary: str, human_context: str) -> str:
+         print("ğŸ¯ Step 2: Refining with human interview responses...")
+         
          # Add context about what we're doing
-         refinement_context = f"""# Initial Human-Based Summary
+         refinement_context = f"""# Initial AI-Based Summary
  {current_summary}
  
- # AI-Generated Responses for Refinement
- The following responses were generated by AI using the human interview answers as context/seeds for vector search and RAG. Use these to refine and enhance the initial summary with deeper insights while maintaining the authentic human foundation.
+ # Human Interview Responses for Refinement
+ The following responses are authentic human answers. Use these to refine and ground the initial AI summary with real human authenticity while enhancing with AI insights.
  
- {ai_context}
+ {human_context}
  """
- 
+         
          # Process context with token-based chunking
          config_params = {"existing_answer": current_summary}
```

**Chunk 2**
Lines added: 21, lines removed: 21

```diff
@@ -654,44 +609,44 @@
              max_tokens=30000
          )
- 
+         
          return summary
- 
-     # Run the processing with human foundation + AI refinement workflow
-     async def run_human_plus_ai_creation():
+     
+     # Run the processing with AI foundation + human refinement workflow
+     async def run_ai_plus_human_creation():
          step_count = 0
          initial_bio = ""
          final_summary = ""
- 
+         
          # Initialize file paths (will be set during processing)
          bio_path = None
          output_path = None
- 
-         # Step 1: Generate initial summary from HUMAN data
-         initial_bio = await generate_initial_summary(human_context)
+         
+         # Step 1: Generate initial summary from AI data
+         initial_bio = await generate_initial_summary(ai_context)
          print(f"Step 1 Result: {len(initial_bio)} characters generated")
- 
-         # Save intermediate bio immediately after generation
-         bio_filename = "human_interview_bio.md"
+         
+         # Save intermediate bio immediately after generation (rename to reflect AI base)
+         bio_filename = "ai_interview_bio.md"  # Changed filename to reflect AI initial
          bio_path = config.paths.OUTPUT_DIR / bio_filename
-         print(f"ğŸ’¾ Saving human interview bio to {bio_path}...")
+         print(f"ğŸ’¾ Saving AI initial bio to {bio_path}...")
          try:
              with open(bio_path, "w", encoding="utf-8") as f:
                  f.write(initial_bio)
-             print(f"âœ… Successfully saved human interview bio ({len(initial_bio)} characters)")
+             print(f"âœ… Successfully saved AI initial bio ({len(initial_bio)} characters)")
          except Exception as e:
-             print(f"âŒ Error saving human interview bio: {e}")
- 
-         # Step 2: Refine with AI responses
-         final_summary = await refine_with_ai_responses(initial_bio, ai_context)
+             print(f"âŒ Error saving AI initial bio: {e}")
+         
+         # Step 2: Refine with human responses
+         final_summary = await refine_with_human_responses(initial_bio, human_context)
          print(f"Step 2 Result: {len(final_summary)} characters generated")
- 
+         
          # Save final prompt to assistant directory
          output_filename = "main.md"
          output_path = config.paths.ASSISTANT_PROMPTS_DIR / output_filename
- 
+         
          # Ensure assistant directory exists
          output_path.parent.mkdir(parents=True, exist_ok=True)
- 
-         print(f"ğŸ’¾ Saving final human+AI refined prompt to {output_path}...")
+         
+         print(f"ğŸ’¾ Saving final AI+human refined prompt to {output_path}...")
          try:
              with open(output_path, "w", encoding="utf-8") as f:
```

**Chunk 3**
Lines added: 9, lines removed: 9

```diff
@@ -706,23 +661,23 @@
          except Exception as e:
              print(f"âŒ Error saving final prompt: {e}")
- 
+         
          step_count = 2
- 
+         
          # Ensure all paths are set with fallbacks
          if bio_path is None:
-             bio_filename = "human_interview_bio.md"
+             bio_filename = "ai_interview_bio.md"
              bio_path = config.paths.OUTPUT_DIR / bio_filename
          if output_path is None:
              output_filename = "main.md"
              output_path = config.paths.ASSISTANT_PROMPTS_DIR / output_filename
- 
+         
          print(f"Files created:")
-         print(f"  ğŸ“„ Initial Bio (Human): {bio_path}")
-         print(f"  ğŸ¯ Final Human+AI Prompt: {output_path}")
+         print(f"  ğŸ“„ Initial Bio (AI): {bio_path}")
+         print(f"  ğŸ¯ Final AI+Human Prompt: {output_path}")
          print(f"\nProcess completed in {step_count} steps with {step_count} AI calls.")
-         print("ğŸ‘¤ğŸ¤– System prompt built from human foundation + AI refinement!")
- 
+         print("ğŸ¤–ğŸ‘¤ System prompt built from AI foundation + human refinement!")
+     
      import asyncio
-     asyncio.run(run_human_plus_ai_creation())
+     asyncio.run(run_ai_plus_human_creation())
  
  def create_baseline_prompts():
```

</details>

---

Yes, I've updated the `prompt_creator.py` script in human mode to swap the data sources as you described: now it generates the initial summary from the AI-generated responses (from the most recent `questions_with_answers_songbird_*.csv`), and then refines that with the human interview answers (from the most recent `human_interview_*.csv`). 

I also adjusted the intermediate output filename to `ai_interview_bio.md` (saved in the `output/` directory) to better reflect the new initial data source, while keeping the final refined prompt as `main.md` in the `prompts/parts/assistant/` directory. The overall structure and workflow remain the same.

If you'd like to test it, you can run:
```
python prompt_creator.py --mode human
```

Let me know if this works as expected or if you need any further tweaks (e.g., to filenames, prompts, or other details)!

---

_**User (2025-09-26 11:28Z)**_

Are we clamping the tokens to 30,000 for a reason? It seems like we could let the intial prompt be longer

---

_**Assistant**_

<details><summary>Edit file: /home/codyt/Documents/Projects/Cognitive-Assistant/Existential Layer/prompt_creator.py</summary>

undefined

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -582,5 +582,5 @@
              prompt_template=initial_summary_template,
              config_params=config_params,
-             max_tokens=30000
+             max_tokens=100000  # Increased to allow longer initial prompts
          )
          
```

</details>

---

