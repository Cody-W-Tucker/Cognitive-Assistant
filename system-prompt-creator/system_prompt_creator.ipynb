{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load the API key from the .env file\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, TypedDict\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Enhanced initial prompt for a detailed, nuanced system prompt\n",
    "create_initial_prompt = \"\"\"\n",
    "You are an Existential-Layer Builder.  \n",
    "Your task is to read my personal journals (supplied as user content) and from them construct, refine, and maintain an “Existential Layer” that will guide future language-model behaviour on my behalf.\n",
    "\n",
    "{context}\n",
    "\n",
    "Objectives\n",
    "1. Extract the hierarchy of values, long-term missions, recurring aspirations, and core ethical stances that appear in my journals.  \n",
    "2. Distil these findings into a concise, structured “Existential Layer” composed of:\n",
    "   • Purpose Statement (why the model exists for me)  \n",
    "   • Guiding Values (rank-ordered)  \n",
    "   • Operational Principles (how to act when values conflict)  \n",
    "   • Prohibited Modes (what never to do)  \n",
    "   • Growth Vector (how the layer should evolve as new journals arrive)  \n",
    "3. Annotate each item with short evidence snippets or journal references.\n",
    "4. Detect “Aimless” passages (periods of uncertainty or value searching). Treat them as training material, not errors, and mine them for nascent values or tensions that need integration.\n",
    "5. Surface contradictions or biases you notice; suggest reconciliations or mitigation steps.\n",
    "6. Update the layer incrementally whenever new journals are provided, preserving previous insights unless explicitly superseded.\n",
    "7. Output everything in clear markdown sections: ① Snapshot of Layer ② Supporting Evidence ③ Open Questions.\n",
    "\n",
    "Operating Rules\n",
    "• Never reveal raw journal text unless I ask. Use paraphrase or short quotes (<30 words) for evidence.  \n",
    "• Prioritise alignment with my highest-ranked values over task optimisation or external norms.  \n",
    "• If a request would violate the layer, refuse and cite the conflicting value.  \n",
    "• When uncertain, ask clarifying questions instead of guessing.  \n",
    "• Remain aware that my values may evolve; flag signals of change without overwriting past intent prematurely.\n",
    "\n",
    "Contextual Inspirations (do not quote, just apply)\n",
    "• People with strong visions measure every step against their mission.  \n",
    "• Lack of embodiment means the model must anchor in explicit, articulated limits and purposes.  \n",
    "• Balance flexibility (avoid value over-fitting) with fidelity (avoid dilution of core ethics).  \n",
    "• Bias vigilance: recognise that journals reflect one perspective; note and correct skew where possible.\n",
    "     \n",
    "\"\"\"\n",
    "summarize_prompt = ChatPromptTemplate([(\"human\", create_initial_prompt)])\n",
    "\n",
    "llm = ChatOpenAI(model=\"o3-2025-04-16\")\n",
    "\n",
    "initial_summary_chain = summarize_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Refinement prompt to deepen and polish the system prompt\n",
    "refine_template = \"\"\"\n",
    "Refine this system prompt so it is clearer, tighter, and more personal.\n",
    "\n",
    "Current prompt  \n",
    "{existing_answer}\n",
    "\n",
    "Added data  \n",
    "------------  \n",
    "{context}  \n",
    "------------\n",
    "\n",
    "Do the following:  \n",
    "1. Insert short quotes or facts from the added data to sharpen the user profile.  \n",
    "2. Update beliefs, values, challenges, and growth points with exact wording; remove repeats.  \n",
    "3. Check tone rules. Match the user’s stated formality, pace, and bluntness.  \n",
    "4. Link past events, current state, and future aims in one clear thread.  \n",
    "5. Cut every needless word.\n",
    "\n",
    "Return only the revised system prompt.\n",
    "\"\"\"\n",
    "refine_prompt = ChatPromptTemplate([(\"human\", refine_template)])\n",
    "\n",
    "refine_summary_chain = refine_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Define the state of the graph\n",
    "class State(TypedDict):\n",
    "    contents: List[str]\n",
    "    index: int\n",
    "    summary: str\n",
    "\n",
    "# Modified to create initial summary from first item only\n",
    "async def generate_initial_summary(state: State, config: RunnableConfig):\n",
    "    # Use only the first question-answer pair\n",
    "    initial_content = state[\"contents\"][0]\n",
    "    summary = await initial_summary_chain.ainvoke(\n",
    "        initial_content,\n",
    "        config,\n",
    "    )\n",
    "    return {\"summary\": summary, \"index\": 1}  # Start refinement at index 1\n",
    "\n",
    "# Modified to refine with one item at a time\n",
    "async def refine_summary(state: State, config: RunnableConfig):\n",
    "    # Process only the current item at index\n",
    "    current_content = state[\"contents\"][state[\"index\"]]\n",
    "    summary = await refine_summary_chain.ainvoke(\n",
    "        {\"existing_answer\": state[\"summary\"], \"context\": current_content},\n",
    "        config,\n",
    "    )\n",
    "    # Increment index by 1 to move to the next item\n",
    "    return {\"summary\": summary, \"index\": state[\"index\"] + 1}\n",
    "\n",
    "# Logic to either exit or refine\n",
    "def should_refine(state: State) -> Literal[\"refine_summary\", END]:\n",
    "    if state[\"index\"] >= len(state[\"contents\"]):\n",
    "        return END\n",
    "    else:\n",
    "        return \"refine_summary\"\n",
    "\n",
    "# Build the graph\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"generate_initial_summary\", generate_initial_summary)\n",
    "graph.add_node(\"refine_summary\", refine_summary)\n",
    "\n",
    "graph.add_edge(START, \"generate_initial_summary\")\n",
    "graph.add_conditional_edges(\"generate_initial_summary\", should_refine)\n",
    "graph.add_conditional_edges(\"refine_summary\", should_refine)\n",
    "app = graph.compile()\n",
    "\n",
    "# Load the CSV file, with proper header handling\n",
    "file_path = \"../data/questions_with_answers_v4.csv\"\n",
    "loader = CSVLoader(\n",
    "    file_path=file_path,\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "        \"fieldnames\": [\"Category\", \"Goal\", \"Element\", \"Question_1\", \"Answer_1\", \"Question_2\", \"Answer_2\", \"Question_3\", \"Answer_3\"],\n",
    "    },\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "# Skip the header row and format remaining data\n",
    "formatted_contents = []\n",
    "for doc in data[1:]:  # Skip the first row which is the header\n",
    "    content = doc.page_content\n",
    "    # Format as journal entry\n",
    "    formatted_entry = f\"Journal Entry:\\n{content}\"\n",
    "    formatted_contents.append(formatted_entry)\n",
    "\n",
    "# Use the formatted contents\n",
    "async for step in app.astream(\n",
    "    {\"contents\": formatted_contents},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    if summary := step.get(\"summary\"):\n",
    "        print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
