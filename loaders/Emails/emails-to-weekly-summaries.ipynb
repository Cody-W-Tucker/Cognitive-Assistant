{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in to pandas dataframe the data from the csv file\n",
    "import pandas as pd\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "import constants\n",
    "import os\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv(\"../data/Mail/Important/cleaned_messages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the dataframe named \"Week\"\n",
    "# This column will contain the week number of the year for each date in the \"Date\" column\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
    "\n",
    "# Add a new column to the dataframe named \"Year\"\n",
    "# This column will contain the year for each date in the \"Date\" column\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the formatted date and day of week\n",
    "df_formatted = df.copy()\n",
    "df_formatted[\"FormattedDate\"] = df[\"Date\"].dt.strftime(\"%A, %Y-%m-%d\")\n",
    "df_formatted[\"DayOfWeek\"] = df[\"Date\"].dt.dayofweek\n",
    "\n",
    "# Sort the DataFrame by year, week, and date\n",
    "df_sorted = df_formatted.sort_values([\"Year\", \"Week\", \"Date\"])\n",
    "\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = constants.APIKEY\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"Compose a detailed summary based on the provided list of business correspondence emails exchanged between me and my customers during the given week. Your mission is to give a report on everything that happened in the week, include what happened and who was involved.\n",
    "Emails: {page_content}\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# Chain\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"Generate a comprehensive summary by significant events of the individual summaries created in the previous task. Ensure the summary remains concise, insightful, and suitable for a weekly review, with a target length of around 3000 characters.\n",
    "Individual Summaries: {doc_summaries}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory\n",
    "dir_name = \"Emails\"\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "# Load the entire DataFrame\n",
    "emails = df_sorted\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "emails.to_csv(\"emails.csv\", index=False)\n",
    "\n",
    "loader = CSVLoader(\"emails.csv\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Print confirmation of load\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"page_content\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Group the dataframe by Year and Week\n",
    "grouped = df_sorted.groupby([\"Year\", \"Week\"])\n",
    "\n",
    "# Create a DataFrame to store all results\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Process each group with the map reduce chain\n",
    "for name, group in grouped:\n",
    "    year, week = name\n",
    "    filename = os.path.join(dir_name, f\"Year_{year}_Week_{week}.csv\")\n",
    "\n",
    "    # Save the group to a CSV file\n",
    "    group.to_csv(filename, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    loader = CSVLoader(filename)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split the documents\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Apply the map reduce chain\n",
    "    result = map_reduce_chain(split_docs)\n",
    "    output_text = result.get(\"output_text\", None)\n",
    "    intermediate_steps = result.get(\"intermediate_steps\", None)\n",
    "\n",
    "    # Create a DataFrame from the result and append it to all_results\n",
    "    result_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Year\": [year],\n",
    "            \"Week\": [week],\n",
    "            \"Output Text\": [output_text],\n",
    "            \"Intermediate Steps\": [str(intermediate_steps)],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Remove line breaks from the output text and intermediate steps\n",
    "    result_df[\"Output Text\"] = result_df[\"Output Text\"].str.replace(\"\\n\", \" \")\n",
    "    result_df[\"Intermediate Steps\"] = result_df[\"Intermediate Steps\"].str.replace(\n",
    "        \"\\n\", \" \"\n",
    "    )\n",
    "\n",
    "    all_results = pd.concat([all_results, result_df], ignore_index=True)\n",
    "\n",
    "    # Save all_results to a CSV file\n",
    "    all_results.to_csv(\"all_results.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    if output_text:\n",
    "        print(f\"Output Text for Year {year} Week {week}:\")\n",
    "        print(output_text)\n",
    "\n",
    "    if intermediate_steps:\n",
    "        print(f\"Intermediate Steps for Year {year} Week {week}:\")\n",
    "        for step in intermediate_steps:\n",
    "            print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "df = pd.read_csv(\"all_results.csv\")\n",
    "\n",
    "dir_name = \"Summaries\"\n",
    "\n",
    "# Create a directory for all files\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    year = row[\"Year\"]  # Assuming 'Year' column exists\n",
    "    week = row[\"Week\"]  # Assuming 'Week' column exists\n",
    "    output_text = row[\"Output Text\"]  # Assuming 'Email' column contains the text\n",
    "\n",
    "    # Create a directory for each year\n",
    "    year_dir = os.path.join(dir_name, f\"{year}\")\n",
    "    if not os.path.exists(year_dir):\n",
    "        os.makedirs(year_dir)\n",
    "\n",
    "    # Define the filename with the year directory\n",
    "    filename = os.path.join(year_dir, f\"Week_{week}.md\")\n",
    "\n",
    "    with open(filename, \"w\") as file:\n",
    "        # Write the output text to the file\n",
    "        if output_text:\n",
    "            file.write(f\"# Email Summary Week {week}:\\n\\n\")\n",
    "            file.write(output_text)\n",
    "            file.write(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
